The goal is to figure out and learn things!

For forward pass

Note:- forward()

Creating a squence on GPU, GPU reserves VRAM for block of 'n' integers
It uses Single Instruction, Multiple Data (SIMD) for parallelism.

From the original paper we can we that they have added the two embeddings, 
we want to do that cause a single embedding will not hold the order. 

token_embedding_table = self.token_embedding_table(x) 

example
If embedding_dim = 250
x = torch.tensor([[5, 23, 17]])  # shape: (1, 3)
token_embeddings = embedding_layer(x)  # shape: (1, 3, 250)

Embedding Matrix Shape: (vocab_size, embedding_dim) = (1000, 512)


Input tokens: ["the", "cat", "sat"]  (assuming IDs 5, 17, 23)

Token embeddings shape: (1, 3, 250)

Batch 0 (only batch):
    Token 0: ID=5 ("the") → 250 features:
    [0.152, -0.423, 0.087, 0.654, -0.231, 0.009, -0.543, 0.765, 0.198, -0.032, ... (240 more)]

    Token 1: ID=17 ("cat") → 250 features:
    [0.821, 0.103, 0.312, -0.112, 0.456, 0.789, -0.234, 0.098, 0.543, 0.210, ... (240 more)]

    Token 2: ID=23 ("sat") → 250 features:
    [0.112, -0.345, 0.876, 0.543, -0.098, 0.654, 0.321, -0.765, 0.432, 0.111, ... (240 more)]


Similar to token embedding 

example
pos = torch.arange(3)  # [0, 1, 2]
pos_embeddings = pos_embedding_layer(pos)  # shape: (3, 250)
Position 0 → vector1, Position 1 → vector2, Position 2 → vector3 why because transformers process all tokens in parallel 


Triangular masking
-> It prevents tokens from seeing future tokens during attention.
-> We don't want later words to influence earlier words cause it would give away the answer.


For example:- 

Wihtout masking
Sentence: "The cat ___ on the mat"
Token "___" can see ALL tokens: ["The", "cat", "on", "the", "mat"]
This is like cheating - it knows what comes next!

With masking
Sentence: "The cat ___ on the mat"
Token "___" can ONLY see: ["The", "cat"] ← Past tokens only!
It CANNOT see: ["on", "the", "mat"] ← Future tokens
This is realistic for generation!

In matrix way:-
Maybe like one-hot encode but we set them to -Inf before the softmax
Mask = [
    [1, 0, 0, 0, 0],  # Token 0 can only attend to token 0
    [1, 1, 0, 0, 0],  # Token 1 can attend to tokens 0,1
    [1, 1, 1, 0, 0],  # Token 2 can attend to tokens 0,1,2
    [1, 1, 1, 1, 0],  # Token 3 can attend to tokens 0,1,2,3
    [1, 1, 1, 1, 1],  # Token 4 can attend to all tokens
]
# 1 = allowed, 0 = masked (set to -inf before softmax)


Next step Multi Headed Attention!
Every head in the attention produces a change
we sum together all the pruposed change, one for each head,
and then add that to the original embeddings

MultiHead(Q, K, V ) = Concat(head1, ..., headh)WO

Again for the note:

B,T,C = Batch size,
 sequence length (number of token in each sequence),
 Channel (size of vector)

Input dimenstion is 512:
-> we want to split those into 8 x 64 dim
-> it's going to procress that head in parallel
-> and the output of each head will be of 64 dimenstion
-> The final output will be of the same size.


# combining of multiple head that does the work in parallel and final output
Head 1 slice: [1.0, 2.0, 3.0]  ← Dimensions 0-2
Head 2 slice: [4.0, 5.0, 6.0]  ← Dimensions 3-5

applying Wo, is a 6x6 matrix that learns to mix dimenstions
Wᴼ = [[0.5, 0.2, 0.1, 0.0, 0.0, 0.0],   # Output dim 0 mostly from Head 1
      [0.1, 0.5, 0.1, 0.1, 0.1, 0.1],   # Output dim 1 mix of both heads
      [0.0, 0.0, 0.0, 0.5, 0.2, 0.1],   # Output dim 2 mostly from Head 2
      [0.2, 0.2, 0.2, 0.2, 0.2, 0.2],   # Output dim 3 equal mix
      [0.1, 0.2, 0.3, 0.1, 0.2, 0.3],   # Output dim 4 complex mix
      [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]]   # Output dim 5 weighted toward Head 2




Artitecture of MLP 
Input (d_model=512)
    ↓ Linear layer: 512 → 2048 (4× expansion)
    ↓ ReLU activation (non-linearity)  
    ↓ Linear layer: 2048 → 512 (back to original)
Output (d_model=512)

Now the goal is to undersnad how does this thing work!

Let's take a example of the word bank

It can have a different meaning like river bank, bank angle, or money bank.

In ReLu activation the thinking happens, 
neuron get's activated if the model reconzies the pattern else not


example why do we add the original vector after output

# Original: ambiguous "bank"
original = [0.5, 0.3, 0.4, 0.6, 0.2, ...]

# After MLP: clarified "money bank"  
mlp_output = [0.8, 0.1, 0.7, 0.9, ...]

# Residual: original + mlp_output
result = [1.3, 0.4, 1.1, 1.5, ...]  # Combined!

why do add temp is that block, just for the note

Without residual: x_new = f(x)     ← Must learn everything from scratch
With residual:    x_new = x + f(x) ← Only needs to learn the "delta" (change)



# How does a model calculates loss in character level 

let's say we have a vocab_size of 8
vocab_size = {a,b,c,d,e,f,g,h}

at each step model will output 8 probalities,
let's say we expect the model to produce "abc"

at start: (time 1)
probalities = [0.2, 0.5, 0.1, 0.05, 0.05, 0.05, 0.03, 0.02]

we want the first character to be 'a' the model confidence is 0.2
so we have a loss

given that the a is predicted in next character (time 2)

p = [0.1, 0.6, ....]
we have a loss, conidering the rest of the probalities is smaller than 0.6

time(3) 
p  = [0.05, 0.1, 0.7, ....]
we have a loss again, considering the rest of the probalities is smaller than 0.7

After that it will learn iteratively.
