For forward pass

Note:- forward()

Creating a squence on GPU, GPU reserves VRAM for block of 'n' integers
It uses Single Instruction, Multiple Data (SIMD) for parallelism.

From the original paper we can we that they have added the two embeddings, 
we want to do that cause a single embedding will not hold the order. 

token_embedding_table = self.token_embedding_table(x) 

example
If embedding_dim = 250
x = torch.tensor([[5, 23, 17]])  # shape: (1, 3)
token_embeddings = embedding_layer(x)  # shape: (1, 3, 250)

Embedding Matrix Shape: (vocab_size, embedding_dim) = (1000, 512)


Input tokens: ["the", "cat", "sat"]  (assuming IDs 5, 17, 23)

Token embeddings shape: (1, 3, 250)

Batch 0 (only batch):
    Token 0: ID=5 ("the") → 250 features:
    [0.152, -0.423, 0.087, 0.654, -0.231, 0.009, -0.543, 0.765, 0.198, -0.032, ... (240 more)]

    Token 1: ID=17 ("cat") → 250 features:
    [0.821, 0.103, 0.312, -0.112, 0.456, 0.789, -0.234, 0.098, 0.543, 0.210, ... (240 more)]

    Token 2: ID=23 ("sat") → 250 features:
    [0.112, -0.345, 0.876, 0.543, -0.098, 0.654, 0.321, -0.765, 0.432, 0.111, ... (240 more)]


Similar to token embedding 

example
pos = torch.arange(3)  # [0, 1, 2]
pos_embeddings = pos_embedding_layer(pos)  # shape: (3, 250)
Position 0 → vector1, Position 1 → vector2, Position 2 → vector3 why because transformers process all tokens in parallel 


Triangular masking
-> It prevents tokens from seeing future tokens during attention.
-> We don't want later words to influence earlier words cause it would give away the answer.


For example:- 

Wihtout masking
Sentence: "The cat ___ on the mat"
Token "___" can see ALL tokens: ["The", "cat", "on", "the", "mat"]
This is like cheating - it knows what comes next!

With masking
Sentence: "The cat ___ on the mat"
Token "___" can ONLY see: ["The", "cat"] ← Past tokens only!
It CANNOT see: ["on", "the", "mat"] ← Future tokens
This is realistic for generation!

In matrix way:-
Maybe like one-hot encode but we set them to -Inf before the softmax
Mask = [
    [1, 0, 0, 0, 0],  # Token 0 can only attend to token 0
    [1, 1, 0, 0, 0],  # Token 1 can attend to tokens 0,1
    [1, 1, 1, 0, 0],  # Token 2 can attend to tokens 0,1,2
    [1, 1, 1, 1, 0],  # Token 3 can attend to tokens 0,1,2,3
    [1, 1, 1, 1, 1],  # Token 4 can attend to all tokens
]
# 1 = allowed, 0 = masked (set to -inf before softmax)


Next step Multi Headed Attention!
